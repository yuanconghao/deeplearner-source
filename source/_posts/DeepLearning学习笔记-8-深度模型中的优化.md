---
layout: post
title: DeepLearning学习笔记-8.深度模型中的优化
date: 2022-05-17 16:17:55
toc: true
mathjax: true
categories:
    - 深度学习

tags:
    - 深度学习笔记
    - 花书
---

深度学习算法在多数情况下都涉及优化，而在其涉及的诸多优化问题中，最难的是神经网络训练。本篇主要关注这一类特定的优化问题：寻找神经网络上的一组参数$\theta$，它能够显著地降低代价函数$J(\theta)$，该代价函数通常包括整个训练集上的性能评估和额外的正则化项。更高级的优化算法能够在训练中自适用调整学习率护着使用代价函数二阶导数包含的信息。

<!--more-->

### 学习与纯优化区别
#### 经验风险最小化
#### 代理损失函数和提前终止
#### 批量算法与小批量算法

### 神经网络优化中的挑战
#### 病态
#### 局部极小值
#### 高原、鞍点和其他平坦区域
#### 悬崖与梯度爆炸
#### 长期依赖
#### 非精确梯度
#### 局部和全局结构间的弱对应
#### 优化的理论限制

### 基本算法
#### 随机梯度下降
#### 动量
#### Nesterov动量
#### 参数初始化策略
#### 自适用学习率算法
##### AdaGrad
##### RMSProp
##### Adam
##### 选择正确的优化算法

### 二阶近似方法
#### 牛顿法
#### 共轭梯度
#### BFGS

### 优化策略和元算法
#### 批标准化
#### 坐标下降
#### Polyak平均
#### 监督预训练
#### 设计有助于优化的模型
#### 延拓法和课程学习

